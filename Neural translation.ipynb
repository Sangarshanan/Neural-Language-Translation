{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Translation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('Tamil.txt', 'r+', encoding=\"utf8\")\n",
    "x = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('English.txt', 'r+', encoding=\"utf8\")\n",
    "y = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am using only 500 examples to train pretty fast (I'm lazy FYI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = y[-500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x[0]= x[0].strip('\\ufeffMMA')\n",
    "y[0]= y[0].strip('\\ufeffMMA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "for i in range(0,len(x)):\n",
    "    x[i] = x[i].strip('\\n')\n",
    "    x[i] = ''.join(ch for ch in x[i] if ch not in exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(y)):\n",
    "    y[i] = y[i].lower()\n",
    "    y[i] = y[i].strip('\\n')\n",
    "    y[i] = ''.join(ch for ch in y[i] if ch not in exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamil Text: தன்னுடைய இராஜிநாமாவை பற்றிய அறிக்கையில் பெக் சதித்திட்டங்கள் பற்றியும் அவர் எதிர்கொண்ட வேண்டுமென்றே கொடுக்கப்பட்ட தவறான தகவல்கள் பற்றியும் குறிப்பிட்டுள்ளார் \n",
      "\n",
      "English Text: in a personal statement about his resignation beck spoke about the plots and deliberate misinformation he had faced\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamil Text:\",x[1],\"\\n\")\n",
    "print(\"English Text:\",y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_words = []\n",
    "for i in range(0,len(y)):\n",
    "    english_words.append(y[i].split())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_words = [j for sub in english_words for j in sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique English words: 3448\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Unique English words:\",len(set(english_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tamil_words = []\n",
    "for i in range(0,len(x)):\n",
    "    tamil_words.append(x[i].split())  \n",
    "tamil_words = [j for sub in tamil_words for j in sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tamil words: 5260\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Unique Tamil words:\",len(set(tamil_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tamilvocab = len(set(tamil_words))\n",
    "engvocab = len(set(english_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks like there are more number of unique words in Tamil.....as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length_tamil=[]\n",
    "for i in range(0,len(x)):\n",
    "    length_tamil.append(len(x[i].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length_english=[]\n",
    "for i in range(0,len(y)):\n",
    "    length_english.append(len(y[i].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average number of words in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.11"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(length_english)/len(length_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.052"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(length_tamil)/len(length_tamil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "print(max(length_english))\n",
    "print(max(length_tamil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "english_words_counter = collections.Counter([word for sentence in y for word in sentence.split()])\n",
    "tamil_words_counter = collections.Counter([word for sentence in x for word in sentence.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words in both languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 950),\n",
       " ('of', 442),\n",
       " ('and', 395),\n",
       " ('to', 314),\n",
       " ('in', 296),\n",
       " ('a', 205),\n",
       " ('that', 137),\n",
       " ('is', 129),\n",
       " ('for', 106),\n",
       " ('on', 95)]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_words_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ஒரு', 90),\n",
       " ('என்று', 69),\n",
       " ('மற்றும்', 56),\n",
       " ('இந்த', 44),\n",
       " ('என்ற', 37),\n",
       " ('அவர்', 30),\n",
       " ('அமெரிக்க', 26),\n",
       " ('நான்', 24),\n",
       " ('அவர்கள்', 23),\n",
       " ('என', 23)]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamil_words_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer:\n",
    "\n",
    "Now that our corpus is ready we have to represent it in a way that the neural network can understand, So we convert the text representation to number representation. In words based representation each word his assigned a number abd in character based representation each character is assigned a number. I am using a word level model for its simpler complexity\n",
    "\n",
    "Keras Tokenizer simplifies the representation process for us (This class allows to vectorize a text corpus, by turning each text into either a sequence of integers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Setting gpu use to 0.3 as maximum gpu usage by CUDA results in internal error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "def tokenize(x):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x) \n",
    "    return tokenizer.texts_to_sequences(x), tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'தெற்கு ஈராக்கிலுள்ள பிரிட்டிஷ் படைகள் ஒரு எதிரி இராணுவத்தை எதிர்கொள்ளவில்லை ஆனால் தங்களது நாட்டை வெளிநாட்டவர் ஆக்கிரமிப்பு செய்திருப்பதற்கு ஆழ்ந்த எதிர்ப்பை தெரிவிக்கும் மக்களை அது எதிர்கொண்டுள்ளது'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[444,\n",
       " 927,\n",
       " 445,\n",
       " 270,\n",
       " 1,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 15,\n",
       " 446,\n",
       " 931,\n",
       " 932,\n",
       " 447,\n",
       " 933,\n",
       " 271,\n",
       " 182,\n",
       " 934,\n",
       " 272,\n",
       " 11,\n",
       " 935]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=(tokenize(x))\n",
    "z[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumps': 5, 'over': 6, 'lazy': 7, 'dog': 8, 'by': 9, 'jove': 10, 'my': 11, 'study': 12, 'of': 13, 'lexicography': 14, 'won': 15, 'a': 16, 'prize': 17}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 3, 4, 5, 6, 1, 7, 8]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [9, 10, 11, 2, 12, 13, 14, 15, 16, 17]\n"
     ]
    }
   ],
   "source": [
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',]\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding:\n",
    "\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length. Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def pad(x, length=None):\n",
    "    return pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT IS ALWAYS A LENGTH 10 ARRAY....FILLED BY 0s IN THE END\n",
      "Sequence 1 in x\n",
      "  Input:  [1 2 3 4 5 6 1 7 8]\n",
      "  Output: [1 2 3 4 5 6 1 7 8 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [ 9 10 11  2 12 13 14 15 16 17]\n",
      "  Output: [ 9 10 11  2 12 13 14 15 16 17]\n"
     ]
    }
   ],
   "source": [
    "test_pad = pad(text_tokenized)\n",
    "print(\"OUTPUT IS ALWAYS A LENGTH 10 ARRAY....FILLED BY 0s IN THE END\")\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply all the tested preprocessing functions to our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    print('shape before: ', preprocess_y.shape)\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "    print('shape after: ', preprocess_y.shape)\n",
    "        \n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 53)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_x, x_tk = tokenize(x)\n",
    "preprocess_x = pad(preprocess_x)\n",
    "preprocess_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before:  (500, 69)\n",
      "shape after:  (500, 69, 1)\n"
     ]
    }
   ],
   "source": [
    "preproc_tamil_sentences, preproc_english_sentences, tamil_tokenizer, english_tokenizer =\\\n",
    "    preprocess(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning a number to each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ஒரு', 1), ('என்று', 2), ('மற்றும்', 3), ('இந்த', 4), ('என்ற', 5)]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tamil_tokenizer.word_index.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1), ('of', 2), ('and', 3), ('to', 4), ('in', 5)]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(english_tokenizer.word_index.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logits to text\n",
    "\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want. We want the Tamil translation. The function logits_to_text will bridge the gab between the logits from the neural network to the Tamil translation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import load_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamil_sentences shape:  (500, 69, 1)\n",
      "english_sentences  shape:  (500, 53)\n",
      "output sequence length:  69\n"
     ]
    }
   ],
   "source": [
    "print(\"tamil_sentences shape: \", preproc_english_sentences.shape)\n",
    "print(\"english_sentences  shape: \", preproc_tamil_sentences.shape)\n",
    "print('output sequence length: ', preproc_english_sentences.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_x = pad(preproc_tamil_sentences, preproc_english_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[-2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 69, 1)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU, Input, Dense, TimeDistributed\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "\n",
    "def simple_model(input_shape, output_sequence_length, tamil_vocab_size, english_vocab_size, learning_rate=0.1):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, dropout=0.1,input_shape=input_shape[1:], return_sequences=True) )\n",
    "    model.add(TimeDistributed(Dense(english_vocab_size, activation='softmax') ))\n",
    "    print('######## Summary ###########')\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Summary ###########\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_5 (GRU)                  (None, 69, 128)           49920     \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 69, 5261)          678669    \n",
      "=================================================================\n",
      "Total params: 728,589\n",
      "Trainable params: 728,589\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      "400/400 [==============================] - 13s 32ms/step - loss: 8.5656 - acc: 0.3063 - val_loss: 8.3063 - val_acc: 0.6686\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 11s 29ms/step - loss: 8.1665 - acc: 0.6669 - val_loss: 8.5247 - val_acc: 0.0322\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 11s 29ms/step - loss: 8.1631 - acc: 0.0603 - val_loss: 5.1870 - val_acc: 0.3538\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 4.9240 - acc: 0.3476 - val_loss: 3.3943 - val_acc: 0.6801\n",
      "Epoch 5/20\n",
      "400/400 [==============================] - 11s 28ms/step - loss: 3.1020 - acc: 0.6801 - val_loss: 3.4255 - val_acc: 0.6626\n",
      "Epoch 6/20\n",
      "400/400 [==============================] - 11s 27ms/step - loss: 3.0953 - acc: 0.6658 - val_loss: 3.7773 - val_acc: 0.6612\n",
      "Epoch 7/20\n",
      "400/400 [==============================] - 11s 28ms/step - loss: 3.3433 - acc: 0.6637 - val_loss: 3.5661 - val_acc: 0.6622\n",
      "Epoch 8/20\n",
      "400/400 [==============================] - 11s 28ms/step - loss: 3.1640 - acc: 0.6651 - val_loss: 3.5378 - val_acc: 0.6626\n",
      "Epoch 9/20\n",
      "400/400 [==============================] - 11s 28ms/step - loss: 3.1108 - acc: 0.6656 - val_loss: 3.4812 - val_acc: 0.6645\n",
      "Epoch 10/20\n",
      "400/400 [==============================] - 11s 28ms/step - loss: 3.1399 - acc: 0.6668 - val_loss: 3.5374 - val_acc: 0.6706\n",
      "Epoch 11/20\n",
      "400/400 [==============================] - 11s 28ms/step - loss: 3.1405 - acc: 0.6732 - val_loss: 3.4180 - val_acc: 0.6799\n",
      "Epoch 12/20\n",
      "400/400 [==============================] - 11s 28ms/step - loss: 3.0188 - acc: 0.6786 - val_loss: 3.4102 - val_acc: 0.6761\n",
      "Epoch 13/20\n",
      "400/400 [==============================] - 11s 28ms/step - loss: 2.9593 - acc: 0.6771 - val_loss: 3.4371 - val_acc: 0.6770\n",
      "Epoch 14/20\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 2.9602 - acc: 0.6767 - val_loss: 3.4638 - val_acc: 0.6665\n",
      "Epoch 15/20\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 2.9899 - acc: 0.6671 - val_loss: 3.5030 - val_acc: 0.6646\n",
      "Epoch 16/20\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 3.2093 - acc: 0.6546 - val_loss: 3.4378 - val_acc: 0.6693\n",
      "Epoch 17/20\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 2.8887 - acc: 0.6705 - val_loss: 3.5227 - val_acc: 0.6628\n",
      "Epoch 18/20\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 2.9966 - acc: 0.6588 - val_loss: 3.4057 - val_acc: 0.6668\n",
      "Epoch 19/20\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 2.8782 - acc: 0.6694 - val_loss: 3.4280 - val_acc: 0.6665\n",
      "Epoch 20/20\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 2.8664 - acc: 0.6707 - val_loss: 3.4127 - val_acc: 0.6722\n"
     ]
    }
   ],
   "source": [
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_english_sentences.shape[1],\n",
    "    engvocab+1,\n",
    "    tamilvocab+1)\n",
    "if os.path.exists(os.path.join(\"cache\", \"simple_model.h5\"))== False:\n",
    "    simple_rnn_model.fit(tmp_x, preproc_english_sentences, batch_size=1024, epochs=20, validation_split=0.2)\n",
    "else:\n",
    "    simple_rnn_model = load_model(os.path.join(\"cache\", \"simple_model.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rnn_model.save(os.path.join(\"model\", \"translate.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accurancy:  0.6741739158630371\n"
     ]
    }
   ],
   "source": [
    "score = simple_rnn_model.evaluate(tmp_x, preproc_english_sentences, verbose=0)\n",
    "print(\"Train accurancy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.9074899463653563, 0.6741739158630371]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pretty decent for a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'தெற்கு ஈராக்கிலுள்ள பிரிட்டிஷ் படைகள் ஒரு எதிரி இராணுவத்தை எதிர்கொள்ளவில்லை ஆனால் தங்களது நாட்டை வெளிநாட்டவர் ஆக்கிரமிப்பு செய்திருப்பதற்கு ஆழ்ந்த எதிர்ப்பை தெரிவிக்கும் மக்களை அது எதிர்கொண்டுள்ளது'"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the the the the petroleum response response response a a a a a a a a a a the a of <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "## PRETTY BAD TRANSLATION :(\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], english_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty sure more data is necessary and also need to penalise stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also it is possible to use word embeddings, encoder-decoder or even Bi-directional LSTM's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "\n",
    "https://github.com/Barqawiz/aind2-nlp-capstone-translation/blob/master/machine_translation.ipynb\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
